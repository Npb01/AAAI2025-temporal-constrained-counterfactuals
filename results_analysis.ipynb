{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "27d31fe5-93bc-45e5-b309-399034c0a08f",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "path = \"results/synthetic_data\"\n",
    "folders = [\"results/synthetic_data/10%\",\n",
    "           \"results/synthetic_data/25%\",\n",
    "           \"results/synthetic_data/50%\"\n",
    "          ]\n",
    "dfs = []\n",
    "for subdir, dirs, files in os.walk(path+'/'):\n",
    "    for file in files:\n",
    "        if 'cfeval' in file:\n",
    "            df = pd.read_csv(os.path.join(subdir,file))\n",
    "            df['alphabet_coverage'] = subdir.split('/')[-1]\n",
    "            dfs.append(df)\n",
    "df1 = pd.concat(dfs)\n",
    "\n",
    "\n",
    "path = \"results/bpic2012_O_ACCEPTED-COMPLETE\"\n",
    "folders = [\"results/bpic2012_O_ACCEPTED-COMPLETE/10%\",\n",
    "           \"results/bpic2012_O_ACCEPTED-COMPLETE/25%\",\n",
    "           \"results/bpic2012_O_ACCEPTED-COMPLETE/50%\"\n",
    "          ] \n",
    "dfs = []\n",
    "for subdir, dirs, files in os.walk(path+'/'):\n",
    "    for file in files:\n",
    "        if 'cfeval' in file:\n",
    "            df = pd.read_csv(os.path.join(subdir,file))\n",
    "            df['alphabet_coverage'] = subdir.split('/')[-1]\n",
    "            dfs.append(df)\n",
    "df2 = pd.concat(dfs)\n",
    "\n",
    "\n",
    "\n",
    "path = \"results/BPIC17_O_ACCEPTED/\"\n",
    "folders = [\"results/BPIC17_O_ACCEPTED/10%\",\n",
    "           \"results/BPIC17_O_ACCEPTED/25%\",\n",
    "           \"results/BPIC17_O_ACCEPTED/50%\"\n",
    "          ]\n",
    "dfs = []\n",
    "for subdir, dirs, files in os.walk(path+'/'):\n",
    "    for file in files:\n",
    "        if 'cfeval' in file:\n",
    "            df = pd.read_csv(os.path.join(subdir,file))\n",
    "            df['alphabet_coverage'] = subdir.split('/')[-1]\n",
    "            dfs.append(df)\n",
    "df3 = pd.concat(dfs)\n",
    "\n",
    "df1.loc[df1['method'] == 'genetic_ltlf_baseline_operators', 'heuristic'] = 'genetic_ltlf_baseline_operators'\n",
    "df2.loc[df2['method'] == 'genetic_ltlf_baseline_operators', 'heuristic'] = 'genetic_ltlf_baseline_operators'\n",
    "df3.loc[df3['method'] == 'genetic_ltlf_baseline_operators', 'heuristic'] = 'genetic_ltlf_baseline_operators'\n",
    "\n",
    "df1.loc[df1['heuristic'] == 'mar', 'heuristic'] = 'mutate_retry_baseline'\n",
    "df2.loc[df2['heuristic'] == 'mar', 'heuristic'] = 'mutate_retry_baseline'\n",
    "df3.loc[df3['heuristic'] == 'mar', 'heuristic'] = 'mutate_retry_baseline'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "141e6814-14bc-4e1c-bd4c-568fe870ab6f",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "import statsmodels.stats.multitest as smm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df1, df2, df3 are your DataFrames for the three datasets\n",
    "dfs = [df1, df2, df3]\n",
    "results = []\n",
    "\n",
    "# Preprocess the data and calculate mean values for grouping\n",
    "for i, df in enumerate(dfs, 1):\n",
    "    df_groupby = df.groupby(['heuristic', 'prefix_length', 'desired_nr_of_cfs', 'alphabet_coverage']).mean(numeric_only=True).reset_index()\n",
    "    df_groupby['dataset'] = df['dataset'].iloc[0]\n",
    "    results.append(df_groupby)\n",
    "\n",
    "# Combine all datasets into a single DataFrame\n",
    "combined_df = pd.concat(results)\n",
    "combined_df = combined_df.rename(columns={'sat_score':'conformance', 'distance_l2j':'distance', \n",
    "                                          'implausibility_sum':'implausibility', 'avg_nbr_changes_per_cf':'sparsity', \n",
    "                                          'diversity_l2j':'diversity'})\n",
    "combined_df['hit_rate'] = combined_df['generated_cfs'] / combined_df['desired_nr_of_cfs']\n",
    "\n",
    "# List of metrics\n",
    "metrics = ['distance', 'sparsity', 'implausibility', 'conformance', 'diversity', 'hit_rate', 'runtime']\n",
    "\n",
    "friedman_results = {}\n",
    "pairwise_results = {}\n",
    "\n",
    "# Perform tests for each dataset and alphabet_coverage\n",
    "for dataset in combined_df['dataset'].unique():\n",
    "    dataset_df = combined_df[combined_df['dataset'] == dataset]\n",
    "    friedman_results[dataset] = {}\n",
    "    pairwise_results[dataset] = {}\n",
    "\n",
    "    for alphabet_coverage in dataset_df['alphabet_coverage'].unique():\n",
    "        ac_df = dataset_df[dataset_df['alphabet_coverage'] == alphabet_coverage]\n",
    "        friedman_results[dataset][alphabet_coverage] = {}\n",
    "        pairwise_results[dataset][alphabet_coverage] = {}\n",
    "\n",
    "        for metric in metrics:\n",
    "            # Ensure uniqueness by grouping and averaging\n",
    "            df_pivot = ac_df.groupby(['prefix_length', 'desired_nr_of_cfs', 'heuristic'])[metric].mean().unstack()\n",
    "\n",
    "            # Drop rows with any NaN values to ensure equal N for all groups\n",
    "            df_pivot = df_pivot.dropna()\n",
    "\n",
    "            if len(df_pivot.columns) < 2:\n",
    "                continue  # Skip if there are less than 2 methods to compare\n",
    "\n",
    "            # Perform Friedman test\n",
    "            stat, p_value = friedmanchisquare(*[df_pivot[method] for method in df_pivot.columns])\n",
    "            friedman_results[dataset][alphabet_coverage][metric] = p_value\n",
    "\n",
    "            if p_value < 0.05:\n",
    "                heuristics = df_pivot.columns\n",
    "                p_values = []\n",
    "                pairs = []\n",
    "\n",
    "                for i in range(len(heuristics)):\n",
    "                    for j in range(i + 1, len(heuristics)):\n",
    "                        h1 = heuristics[i]\n",
    "                        h2 = heuristics[j]\n",
    "                        data_h1 = df_pivot[h1]\n",
    "                        data_h2 = df_pivot[h2]\n",
    "\n",
    "                        if np.array_equal(data_h1, data_h2):\n",
    "                            p_value = 1.0\n",
    "                        else:\n",
    "                            try:\n",
    "                                # Perform Wilcoxon signed-rank test\n",
    "                                stat, p_value = wilcoxon(data_h1, data_h2, zero_method='wilcox')\n",
    "                            except ValueError:\n",
    "                                p_value = np.nan\n",
    "                        p_values.append(p_value)\n",
    "                        pairs.append((h1, h2))\n",
    "\n",
    "                # Perform multiple comparisons correction (e.g., Bonferroni correction)\n",
    "                corrected_p_vals = smm.multipletests(p_values, alpha=0.05, method='bonferroni')[1]\n",
    "                pairwise_results[dataset][alphabet_coverage][metric] = dict(zip(pairs, corrected_p_vals))\n",
    "\n",
    "# Initialize a dictionary to store ranks\n",
    "ranks = {dataset: {alphabet_coverage: {metric: {} for metric in metrics} \n",
    "                   for alphabet_coverage in combined_df['alphabet_coverage'].unique()} \n",
    "         for dataset in combined_df['dataset'].unique()}\n",
    "\n",
    "# Assign ranks based on significant differences\n",
    "for dataset in combined_df['dataset'].unique():\n",
    "    for alphabet_coverage in combined_df['alphabet_coverage'].unique():\n",
    "        dataset_ac_df = combined_df[(combined_df['dataset'] == dataset) & \n",
    "                                    (combined_df['alphabet_coverage'] == alphabet_coverage)]\n",
    "\n",
    "        for metric in metrics:\n",
    "            df_pivot = dataset_ac_df.groupby(['prefix_length', 'desired_nr_of_cfs', 'heuristic'])[metric].mean().unstack()\n",
    "            df_pivot = df_pivot.dropna()\n",
    "\n",
    "            if len(df_pivot.columns) < 2:\n",
    "                continue  # Skip if there are less than 2 methods to compare\n",
    "\n",
    "            methods = df_pivot.columns\n",
    "\n",
    "            if friedman_results[dataset][alphabet_coverage][metric] < 0.05:\n",
    "                for method in methods:\n",
    "                    ranks[dataset][alphabet_coverage][metric][method] = 1  # Initial rank\n",
    "\n",
    "                for (h1, h2), corrected_p_val in pairwise_results[dataset][alphabet_coverage][metric].items():\n",
    "                    if corrected_p_val < 0.05:\n",
    "                        if df_pivot[h1].mean() < df_pivot[h2].mean():\n",
    "                            ranks[dataset][alphabet_coverage][metric][h2] += 1\n",
    "                        else:\n",
    "                            ranks[dataset][alphabet_coverage][metric][h1] += 1\n",
    "            else:\n",
    "                for method in methods:\n",
    "                    ranks[dataset][alphabet_coverage][metric][method] = 1  # Same rank if no significant difference\n",
    "\n",
    "# Create a DataFrame to store the ranks\n",
    "rank_table = pd.DataFrame(columns=['dataset', 'metric', 'alphabet_coverage', 'heuristic', 'rank'])\n",
    "\n",
    "# Populate the DataFrame with ranks\n",
    "for dataset in combined_df['dataset'].unique():\n",
    "    for alphabet_coverage in combined_df['alphabet_coverage'].unique():\n",
    "        dataset_ac_df = combined_df[(combined_df['dataset'] == dataset) & \n",
    "                                    (combined_df['alphabet_coverage'] == alphabet_coverage)]\n",
    "        for metric in metrics:\n",
    "            df_pivot = dataset_ac_df.groupby(['prefix_length', 'desired_nr_of_cfs', 'heuristic'])[metric].mean().unstack()\n",
    "            df_pivot = df_pivot.dropna()\n",
    "            for (prefix_length, desired_nr_of_cfs), row in df_pivot.iterrows():\n",
    "                for heuristic, value in row.items():\n",
    "                    rank_table = rank_table.append({\n",
    "                        'dataset': dataset,\n",
    "                        'metric': metric,\n",
    "                        'alphabet_coverage': alphabet_coverage,\n",
    "                        'heuristic': heuristic,\n",
    "                        'rank': ranks[dataset][alphabet_coverage][metric][heuristic]\n",
    "                    }, ignore_index=True)\n",
    "\n",
    "# Pivot the rank_table to show ranks for each dataset, heuristic, metric, and alphabet coverage\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "50092c30-dd3d-4ca2-9e04-43294f4b78a5",
   "metadata": {},
   "source": [
    "pivot_table = rank_table.pivot_table(index=['dataset', 'alphabet_coverage','metric'], columns='heuristic', values='rank')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "27eec66d-e7f6-491f-bb36-1781ddb67db0",
   "metadata": {},
   "source": [
    "pivot_table.columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b8d4a75d-8132-4a0a-b26e-6f5fd8b58068",
   "metadata": {},
   "source": [
    "from IPython.core.display import HTML\n",
    "pivot_table = pivot_table[['genetic_ltlf_baseline_operators','mutate_retry_baseline', 'heuristic_1',\n",
    "       'heuristic_2']]\n",
    "display(HTML(pivot_table.to_html()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7946908d-acbc-4340-b836-3c40057ecca0",
   "metadata": {},
   "source": [
    "# Assuming df1, df2, df3 are your DataFrames for the three datasets\n",
    "dfs = [df1, df2, df3]\n",
    "results = []\n",
    "for i, df in enumerate(dfs, 1):\n",
    "    df_groupby = df.groupby(['heuristic', 'prefix_length', 'desired_nr_of_cfs']).mean(numeric_only=True).reset_index()\n",
    "    df_groupby['dataset'] = df['dataset'][1].unique()[0]\n",
    "    results.append(df_groupby)\n",
    "\n",
    "\n",
    "# Combine all datasets into a single DataFrame\n",
    "combined_df = pd.concat(results)\n",
    "combined_df = pd.concat(results)\n",
    "combined_df = combined_df.rename(columns={'sat_score':'conformance','distance_l2j':'distance','implausibility_sum':'implausibility'\n",
    "                                       ,'avg_nbr_changes_per_cf':'sparsity','diversity_l2j':'diversity'})\n",
    "combined_df['hit_rate'] = combined_df['generated_cfs']/combined_df['desired_nr_of_cfs']\n",
    "# List of metrics\n",
    "metrics = ['distance', 'sparsity', 'implausibility', 'conformance', 'diversity', 'hit_rate', 'runtime']\n",
    "\n",
    "friedman_results = {}\n",
    "pairwise_results = {}\n",
    "\n",
    "# Perform tests for each dataset\n",
    "for dataset in combined_df['dataset'].unique():\n",
    "    dataset_df = combined_df[combined_df['dataset'] == dataset]\n",
    "    friedman_results[dataset] = {}\n",
    "    pairwise_results[dataset] = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        df_pivot = dataset_df.pivot(index=['prefix_length', 'desired_nr_of_cfs'], columns='heuristic', values=metric)\n",
    "\n",
    "        # Drop rows with any NaN values to ensure equal N for all groups\n",
    "        df_pivot = df_pivot.dropna()\n",
    "\n",
    "        if len(df_pivot.columns) < 2:\n",
    "            continue  # Skip if there are less than 2 methods to compare\n",
    "\n",
    "        # Perform Friedman test\n",
    "        stat, p_value = friedmanchisquare(*[df_pivot[method] for method in df_pivot.columns])\n",
    "        friedman_results[dataset][metric] = p_value\n",
    "\n",
    "        if p_value < 0.05:\n",
    "            heuristics = df_pivot.columns\n",
    "            p_values = []\n",
    "            pairs = []\n",
    "\n",
    "            for i in range(len(heuristics)):\n",
    "                for j in range(i + 1, len(heuristics)):\n",
    "                    h1 = heuristics[i]\n",
    "                    h2 = heuristics[j]\n",
    "                    data_h1 = df_pivot[h1]\n",
    "                    data_h2 = df_pivot[h2]\n",
    "\n",
    "                    if np.array_equal(data_h1, data_h2):\n",
    "                        p_value = 1.0\n",
    "                    else:\n",
    "                        # Perform Wilcoxon signed-rank test\n",
    "                        try:\n",
    "                            stat, p_value = wilcoxon(data_h1, data_h2, zero_method='wilcox')\n",
    "                        except ValueError:\n",
    "                            p_value = np.nan\n",
    "                    p_values.append(p_value)\n",
    "                    pairs.append((h1, h2))\n",
    "\n",
    "            # Perform multiple comparisons correction (e.g., Bonferroni correction)\n",
    "            corrected_p_vals = smm.multipletests(p_values, alpha=0.05, method='bonferroni')[1]\n",
    "            pairwise_results[dataset][metric] = dict(zip(pairs, corrected_p_vals))\n",
    "\n",
    "# Initialize a dictionary to store ranks\n",
    "ranks = {dataset: {metric: {} for metric in metrics} for dataset in combined_df['dataset'].unique()}\n",
    "\n",
    "# Assign ranks based on significant differences\n",
    "for dataset in combined_df['dataset'].unique():\n",
    "    dataset_df = combined_df[combined_df['dataset'] == dataset]\n",
    "\n",
    "    for metric in metrics:\n",
    "        df_pivot = dataset_df.pivot(index=['prefix_length', 'desired_nr_of_cfs'], columns='heuristic', values=metric)\n",
    "\n",
    "        # Drop rows with any NaN values to ensure equal N for all groups\n",
    "        df_pivot = df_pivot.dropna()\n",
    "\n",
    "        if len(df_pivot.columns) < 2:\n",
    "            continue  # Skip if there are less than 2 methods to compare\n",
    "\n",
    "        methods = df_pivot.columns\n",
    "\n",
    "        if friedman_results[dataset][metric] < 0.05:\n",
    "            for method in methods:\n",
    "                ranks[dataset][metric][method] = 1  # Initial rank\n",
    "\n",
    "            for (h1, h2), corrected_p_val in pairwise_results[dataset][metric].items():\n",
    "                if corrected_p_val < 0.05:\n",
    "                    if df_pivot[h1].mean() < df_pivot[h2].mean():\n",
    "                        ranks[dataset][metric][h2] += 1\n",
    "                    else:\n",
    "                        ranks[dataset][metric][h1] += 1\n",
    "        else:\n",
    "            for method in methods:\n",
    "                ranks[dataset][metric][method] = 1  # Same rank if no significant difference\n",
    "\n",
    "# Create a DataFrame to store the ranks\n",
    "rank_table = pd.DataFrame(columns=['dataset', 'metric', 'heuristic', 'rank'])\n",
    "\n",
    "# Populate the DataFrame with ranks\n",
    "for dataset in ranks.keys():\n",
    "    for metric in ranks[dataset].keys():\n",
    "        for method, rank in ranks[dataset][metric].items():\n",
    "            rank_table = rank_table.append({\n",
    "                'dataset': dataset,\n",
    "                'metric': metric,\n",
    "                'heuristic': method,\n",
    "                'rank': rank\n",
    "            }, ignore_index=True)\n",
    "\n",
    "# Pivot the rank_table to show ranks for each dataset, heuristic, and metric\n",
    "pivot_table = rank_table.pivot_table(index=['dataset', 'heuristic'], columns='metric', values='rank')\n",
    "\n",
    "# Display the pivot table\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e3015039-9e2b-4547-8745-0c8124fcf354",
   "metadata": {},
   "source": [
    "pivot_table"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset wise boxplot results",
   "id": "bc0c6164-7a10-425e-b7e3-ff28e43670d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 261,
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "path = \"results/synthetic_data\"\n",
    "folders = [\"results/synthetic_data/10%\",\n",
    "           \"results/synthetic_data/25%\",\n",
    "           \"results/synthetic_data/50%\"]\n",
    "dfs = []\n",
    "for subdir, dirs, files in os.walk(path+'/'):\n",
    "    for file in files:\n",
    "        if 'cfeval' in file:\n",
    "            df = pd.read_csv(os.path.join(subdir,file))\n",
    "            df['alphabet_coverage'] = subdir.split('/')[-1]\n",
    "            dfs.append(df)\n",
    "final_df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "prefixes = [7,9,11,13]\n",
    "df = final_df\n",
    "# Update 'heuristic' column where 'method' is 'genetic'\n",
    "df.loc[df['method'] == 'genetic_ltlf_baseline_operators', 'heuristic'] = 'genetic_ltlf_baseline_operators'\n",
    "\n",
    "# Update 'heuristic' column where 'method' is 'random'\n",
    "\n",
    "df = df[df['prefix_length'].isin(prefixes)]\n",
    "df = df.rename(columns={'sat_score':'trace fitness','distance_l2j':'distance','implausibility_sum':'implausibility'\n",
    "                                       ,'avg_nbr_changes_per_cf':'sparsity','diversity_l2j':'diversity'})\n",
    "all_data = df\n",
    "all_data['hit_rate'] = all_data['generated_cfs']/all_data['desired_nr_of_cfs']\n",
    "\n",
    "rows = 0 \n",
    "coverages = [\"10%\",\"25%\",\"50%\"]\n",
    "fig_1, axes = plt.subplots(nrows=len(coverages), ncols=7, figsize=(35, 16),sharey='col')\n",
    "\n",
    "for coverage in coverages:\n",
    "    df = all_data[all_data['alphabet_coverage'].str.contains(coverage)]\n",
    "    #df['optimization'].replace({'multiobjective_adapted':'AOMO','multiobjective_baseline':'BOMO',\n",
    "    #                           'single_adapted':'AOSO','single_baseline':'BOSO'},inplace=True)\n",
    "    df = df[\n",
    "        ['heuristic','prefix_length','desired_nr_of_cfs','distance', 'sparsity','implausibility','trace fitness','diversity','hit_rate','runtime']]\n",
    "    df_groupby = df.groupby(['heuristic', 'prefix_length','desired_nr_of_cfs']).mean(numeric_only=True)\n",
    "    # crate the figure and axes\n",
    "    #df_groupby.iloc[-32:-16,-1] *= 0.3\n",
    "    for j, i in enumerate(df_groupby.columns):\n",
    "        #df_groupby[[i]].unstack(level=0).plot(ax=axes[rows][j], legend=None, title=i,linewidth=5.0,\n",
    "        #                                      style=['--', '-', '--','-'],\n",
    "                                              #cmap=\"Accent\",\n",
    "        #                                      fontsize=18,color=['red','orange','green','blue']\n",
    "        #                                  )\n",
    "        medianprops = dict(linewidth=3, color='black')\n",
    "        bplot = df_groupby[[i]].boxplot(by='heuristic',ax=axes[rows][j], #legend=None, title=i,#,linewidth=5.0,\n",
    "                                      #style=['--', '-', '--','-'],\n",
    "                                      #cmap=\"Accent\",\n",
    "                                      patch_artist=True,fontsize=18,medianprops=medianprops,\n",
    "                                         return_type='both'#,color=['red','orange','green','blue']\n",
    "                                  )\n",
    "        colors=['red','orange','green','gray','magenta']\n",
    "        for row_key, (ax,row) in bplot.items():\n",
    "            ax.set_xlabel('')\n",
    "            for i,box in enumerate(row['boxes']):\n",
    "                box.set_facecolor(colors[i])\n",
    "\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.yaxis.set_tick_params(labelleft=True,labelsize=22)\n",
    "        ax.set(xlabel=None)\n",
    "        ax.set(xticklabels=[])\n",
    "        ax.set_title(label=ax.title._text,fontsize=22,fontweight='bold')\n",
    "    axes[rows,3].set_title(\"Alphabet coverage \"+coverage+\"\\n\"+'satisfaction rate',fontweight=\"bold\",size=22)\n",
    "    axes[rows,3].set_ylim(0.3,1.1)\n",
    "    axes[rows,5].set_ylim(0.3,1.1)\n",
    "\n",
    "    rows += 1\n",
    "red_patch = mpatches.Patch(color='red', label='genetic',)\n",
    "orange_patch = mpatches.Patch(color='orange', label='genetic_ltlf_baseline_operators')\n",
    "green_patch = mpatches.Patch(color='green', label='h1.Outback')\n",
    "#blue_patch = mpatches.Patch(color='blue', label='h2.Highlander')\n",
    "gray_patch = mpatches.Patch(color='gray', label='h2.Pathfinder')\n",
    "magenta_patch = mpatches.Patch(color='magenta', label='h3.Freelander')\n",
    "fig_1.legend(handles=[red_patch,orange_patch,green_patch,gray_patch,magenta_patch],loc='upper center',\n",
    "        bbox_to_anchor=(0.5, 1.03),\n",
    "        ncol=6,prop={\"size\":27},fontsize=40)\n",
    "fig_1.suptitle('Claim Management',x=0.07,y=0.73,fontsize=40,fontweight='bold',rotation=90)\n",
    "#fig_1.savefig('/Users/andrei/Desktop/PhD/boxplots_overall_ecai_paper_synthetic_log.png', bbox_inches='tight')\n"
   ],
   "id": "3f3f5cff-f43c-4727-95e1-4bb13250eaa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 290,
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "path = \"results/bpic2012_O_ACCEPTED-COMPLETE\"\n",
    "folders = [\"results/bpic2012_O_ACCEPTED-COMPLETE/10%\",\n",
    "           \"results/bpic2012_O_ACCEPTED-COMPLETE/25%\",\n",
    "           \"results/bpic2012_O_ACCEPTED-COMPLETE/50%\"\n",
    "          ]\n",
    "dfs = []\n",
    "for subdir, dirs, files in os.walk(path+'/'):\n",
    "    if subdir in folders:\n",
    "        for file in files:\n",
    "                if 'cfeval' in file:\n",
    "                        df = pd.read_csv(os.path.join(subdir,file))\n",
    "                        df['alphabet_coverage'] = subdir.split('/')[-1]\n",
    "                        dfs.append(df)\n",
    "final_df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "prefixes = [20,25,30,35]\n",
    "df = final_df\n",
    "# Update 'heuristic' column where 'method' is 'genetic'\n",
    "df.loc[df['method'] == 'genetic_ltlf_baseline_operators', 'heuristic'] = 'genetic_ltlf_baseline_operators'\n",
    "\n",
    "df = df[df['prefix_length'].isin(prefixes)]\n",
    "df = df.rename(columns={'sat_score':'trace fitness','distance_l2j':'distance','implausibility_sum':'implausibility'\n",
    "                                       ,'avg_nbr_changes_per_cf':'sparsity','diversity_l2j':'diversity'})\n",
    "all_data = df\n",
    "all_data['hit_rate'] = all_data['generated_cfs']/all_data['desired_nr_of_cfs']\n",
    "\n",
    "rows = 0 \n",
    "coverages = [\"10%\",\"25%\",\n",
    "             \"50%\"\n",
    "            ]\n",
    "fig_1, axes = plt.subplots(nrows=len(coverages), ncols=7, figsize=(35, 16),sharey='col')\n",
    "\n",
    "for coverage in coverages:\n",
    "    df = all_data[all_data['alphabet_coverage'].str.contains(coverage)]\n",
    "    #df['optimization'].replace({'multiobjective_adapted':'AOMO','multiobjective_baseline':'BOMO',\n",
    "    #                           'single_adapted':'AOSO','single_baseline':'BOSO'},inplace=True)\n",
    "    df = df[\n",
    "        ['heuristic','prefix_length','desired_nr_of_cfs','distance', 'sparsity','implausibility','trace fitness','diversity','hit_rate','runtime']]\n",
    "    df_groupby = df.groupby(['heuristic', 'prefix_length','desired_nr_of_cfs']).mean(numeric_only=True)\n",
    "    # crate the figure and axes\n",
    "    #df_groupby.iloc[-32:-16,-1] *= 0.3\n",
    "    for j, i in enumerate(df_groupby.columns):\n",
    "        #df_groupby[[i]].unstack(level=0).plot(ax=axes[rows][j], legend=None, title=i,linewidth=5.0,\n",
    "        #                                      style=['--', '-', '--','-'],\n",
    "                                              #cmap=\"Accent\",\n",
    "        #                                      fontsize=18,color=['red','orange','green','blue']\n",
    "        #                                  )\n",
    "        medianprops = dict(linewidth=3, color='black')\n",
    "        bplot = df_groupby[[i]].boxplot(by='heuristic',ax=axes[rows][j], #legend=None, title=i,#,linewidth=5.0,\n",
    "                                      #style=['--', '-', '--','-'],\n",
    "                                      #cmap=\"Accent\",\n",
    "                                      patch_artist=True,fontsize=18,medianprops=medianprops,\n",
    "                                         return_type='both'#,color=['red','orange','green','blue']\n",
    "                                  )\n",
    "        colors=['red','orange','green','gray','magenta']\n",
    "        for row_key, (ax,row) in bplot.items():\n",
    "            ax.set_xlabel('')\n",
    "            for i,box in enumerate(row['boxes']):\n",
    "                box.set_facecolor(colors[i])\n",
    "\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.yaxis.set_tick_params(labelleft=True,labelsize=22)\n",
    "        ax.set(xlabel=None)\n",
    "        ax.set(xticklabels=[])\n",
    "        ax.set_title(label=ax.title._text,fontsize=22,fontweight='bold')\n",
    "    axes[rows,3].set_title(\"Alphabet coverage \"+coverage+\"\\n\"+'satisfaction rate',fontweight=\"bold\",size=22)\n",
    "    axes[rows,3].set_ylim(0.3,1.1)\n",
    "    axes[rows,5].set_ylim(0.3,1.1)\n",
    "\n",
    "    rows += 1\n",
    "red_patch = mpatches.Patch(color='red', label='genetic',)\n",
    "orange_patch = mpatches.Patch(color='orange', label='genetic_ltlf_baseline_operators')\n",
    "green_patch = mpatches.Patch(color='green', label='h1.Outback')\n",
    "#blue_patch = mpatches.Patch(color='blue', label='h2.Highlander')\n",
    "gray_patch = mpatches.Patch(color='gray', label='h2.Pathfinder')\n",
    "magenta_patch = mpatches.Patch(color='magenta', label='h3.Freelander')\n",
    "fig_1.legend(handles=[red_patch,orange_patch,green_patch,gray_patch,magenta_patch],loc='upper center',\n",
    "        bbox_to_anchor=(0.5, 1.03),\n",
    "        ncol=6,prop={\"size\":27},fontsize=40)\n",
    "fig_1.suptitle('BPIC2012',x=0.07,y=0.73,fontsize=40,fontweight='bold',rotation=90)\n",
    "#fig_1.savefig('/Users/andrei/Desktop/PhD/boxplots_overall_ecai_paper_synthetic_log.png', bbox_inches='tight')\n"
   ],
   "id": "14758120-2a4f-44d2-a3c7-893336cb9426"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 39,
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "path = \"results/BPIC17_O_ACCEPTED\"\n",
    "folders = [\"results/BPIC17_O_ACCEPTED/10%\",\n",
    "           \"results/BPIC17_O_ACCEPTED/25%\",\n",
    "           \"results/BPIC17_O_ACCEPTED/50%\"\n",
    "          ]\n",
    "dfs = []\n",
    "for subdir, dirs, files in os.walk(path+'/'):\n",
    "    if subdir in folders:\n",
    "        for file in files:\n",
    "                if 'cfeval' in file:\n",
    "                        df = pd.read_csv(os.path.join(subdir,file))\n",
    "                        df['alphabet_coverage'] = subdir.split('/')[-1]\n",
    "                        dfs.append(df)\n",
    "final_df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "prefixes = [20,25,30,35]\n",
    "df = final_df\n",
    "# Update 'heuristic' column where 'method' is 'genetic'\n",
    "df.loc[df['method'] == 'genetic_ltlf_baseline_operators', 'heuristic'] = 'genetic_ltlf_baseline_operators'\n",
    "# Update 'heuristic' column where 'method' is 'random'\n",
    "\n",
    "df = df[df['prefix_length'].isin(prefixes)]\n",
    "df = df.rename(columns={'sat_score':'trace fitness','distance_l2j':'distance','implausibility_sum':'implausibility'\n",
    "                                       ,'avg_nbr_changes_per_cf':'sparsity','diversity_l2j':'diversity'})\n",
    "all_data = df\n",
    "all_data['hit_rate'] = all_data['generated_cfs']/all_data['desired_nr_of_cfs']\n",
    "\n",
    "rows = 0 \n",
    "coverages = [\"10%\",\"25%\",\n",
    "             \"50%\"\n",
    "            ]\n",
    "fig_1, axes = plt.subplots(nrows=len(coverages), ncols=7, figsize=(35, 16),sharey='col')\n",
    "\n",
    "for coverage in coverages:\n",
    "    df = all_data[all_data['alphabet_coverage'].str.contains(coverage)]\n",
    "    #df['optimization'].replace({'multiobjective_adapted':'AOMO','multiobjective_baseline':'BOMO',\n",
    "    #                           'single_adapted':'AOSO','single_baseline':'BOSO'},inplace=True)\n",
    "    df = df[\n",
    "        ['heuristic','prefix_length','desired_nr_of_cfs','distance', 'sparsity','implausibility','trace fitness','diversity','hit_rate','runtime']]\n",
    "    df_groupby = df.groupby(['heuristic', 'prefix_length','desired_nr_of_cfs']).mean(numeric_only=True)\n",
    "    # crate the figure and axes\n",
    "    #df_groupby.iloc[-32:-16,-1] *= 0.3\n",
    "    for j, i in enumerate(df_groupby.columns):\n",
    "        #df_groupby[[i]].unstack(level=0).plot(ax=axes[rows][j], legend=None, title=i,linewidth=5.0,\n",
    "        #                                      style=['--', '-', '--','-'],\n",
    "                                              #cmap=\"Accent\",\n",
    "        #                                      fontsize=18,color=['red','orange','green','blue']\n",
    "        #                                  )\n",
    "        medianprops = dict(linewidth=3, color='black')\n",
    "        bplot = df_groupby[[i]].boxplot(by='heuristic',ax=axes[rows][j], #legend=None, title=i,#,linewidth=5.0,\n",
    "                                      #style=['--', '-', '--','-'],\n",
    "                                      #cmap=\"Accent\",\n",
    "                                      patch_artist=True,fontsize=18,medianprops=medianprops,\n",
    "                                         return_type='both'#,color=['red','orange','green','blue']\n",
    "                                  )\n",
    "        colors=['red','orange','green','gray','magenta']\n",
    "        for row_key, (ax,row) in bplot.items():\n",
    "            ax.set_xlabel('')\n",
    "            for i,box in enumerate(row['boxes']):\n",
    "                box.set_facecolor(colors[i])\n",
    "\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.yaxis.set_tick_params(labelleft=True,labelsize=22)\n",
    "        ax.set(xlabel=None)\n",
    "        ax.set(xticklabels=[])\n",
    "        ax.set_title(label=ax.title._text,fontsize=22,fontweight='bold')\n",
    "    axes[rows,3].set_title(\"Alphabet coverage \"+coverage+\"\\n\"+'satisfaction rate',fontweight=\"bold\",size=22)\n",
    "    axes[rows,3].set_ylim(0.3,1.1)\n",
    "    axes[rows,5].set_ylim(0.3,1.1)\n",
    "\n",
    "    rows += 1\n",
    "red_patch = mpatches.Patch(color='red', label='genetic',)\n",
    "orange_patch = mpatches.Patch(color='orange', label='genetic_ltlf_baseline_operators')\n",
    "green_patch = mpatches.Patch(color='green', label='h1.Outback')\n",
    "#blue_patch = mpatches.Patch(color='blue', label='h2.Highlander')\n",
    "gray_patch = mpatches.Patch(color='gray', label='h2.Pathfinder')\n",
    "magenta_patch = mpatches.Patch(color='magenta', label='h3.Freelander')\n",
    "fig_1.legend(handles=[red_patch,orange_patch,green_patch,gray_patch,magenta_patch],loc='upper center',\n",
    "        bbox_to_anchor=(0.5, 1.03),\n",
    "        ncol=6,prop={\"size\":27},fontsize=40)\n",
    "fig_1.suptitle('BPIC17_O_ACCEPTED',x=0.07,y=0.73,fontsize=40,fontweight='bold',rotation=90)\n",
    "#fig_1.savefig('/Users/andrei/Desktop/PhD/boxplots_overall_ecai_paper_synthetic_log.png', bbox_inches='tight')\n"
   ],
   "id": "dcc862a7-ef43-49bf-bfcb-6e0b864f6959"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
