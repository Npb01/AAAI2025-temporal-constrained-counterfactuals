{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Encodings\n",
    "\n",
    "Declare4Py provides several among the main encoding techniques for vectorizing a traces log. These are useful for applying Machine Learning techniques. The encoding classes provided by Declare4Py (see the `Declare4Py.Encodings` package) take as an input a log in a Pandas dataframe format and return a Pandas dataframe whose rows represent a single trace and the columns the extracted features. The Declare4Py encodings are implemented as scikit-learn transformers so it is straightfoward to use them in a Machine Learning pipeline.\n",
    "\n",
    "The tutorial will cover the following points:\n",
    "\n",
    "1. Encodings families:\n",
    "    1. The boolean encoding;\n",
    "    2. The frequency-based encoding;\n",
    "    3. Aggregated encodings;\n",
    "    4. Indexed encodings:\n",
    "        1. The simple-index encoding;\n",
    "        2. The complex-Index encoding;\n",
    "    5. Static Encodings:\n",
    "        1. The first-state encoding;\n",
    "        2. The second-to-last-state encoding;\n",
    "        3. The last-state encoding;\n",
    "    6. The Ngram encoding;\n",
    "    7. The Declare encoding;\n",
    "2. Encoding combinations:\n",
    "    1. The index-latest-payload encoding;\n",
    "3. A Machine Learning pipeline.\n",
    "\n",
    "Before starting how to use the encodings the necessary packages need to be imported.\n",
    "\n",
    "[1]\n",
    "[2]\n",
    "[3]\n",
    "[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import pm4py\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from Declare4Py.Encodings.Aggregate import Aggregate\n",
    "from Declare4Py.Encodings.IndexBased import IndexBased\n",
    "from Declare4Py.Encodings.Static import Static\n",
    "from Declare4Py.Encodings.PreviousState import PreviousState\n",
    "from Declare4Py.Encodings.LastState import LastState\n",
    "from Declare4Py.Encodings.Ngram import Ngram\n",
    "from Declare4Py.Encodings.Declare import Declare"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input format for the `Encodings` classes are logs as Pandas dataframe. Therefore, we import the event log and convert it in a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "from Declare4Py.D4PyEventLog import D4PyEventLog\n",
    "\n",
    "log_path = os.path.join(\"../../../\", \"tests\", \"test_logs\", \"Sepsis Cases.xes.gz\")\n",
    "event_log = D4PyEventLog(case_name=\"case:concept:name\")\n",
    "event_log.parse_xes_log(log_path)\n",
    "case_id_key = event_log.get_case_name()\n",
    "event_log.to_dataframe()\n",
    "df = event_log.log\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodings families\n",
    "\n",
    "A Declare4Py encoding is implemented as a scikit-learn transformer class, you just need to instantiate the corresponding `encoder` object and call the function `fit_transform(df)` on the input dataframe. The name of the features can be retrieved with the `get_feature_names()` function.\n",
    "\n",
    "### The Boolean Encoding\n",
    "\n",
    "In the __boolean encoding__ sequences of events are represented as feature vectors, in such a way that each feature corresponds to an event class (an activity) from the log. This is achieved with the `Declare4Py.Encodings.Aggregate.Aggregate` class by setting the categorical attributes and the `boolean` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "encoder = Aggregate(case_id_col=case_id_key, cat_cols=['concept:name', 'org:group'], boolean=True)\n",
    "enc_df = encoder.fit_transform(df)\n",
    "\n",
    "print(f\"Log features:\\n {encoder.get_feature_names()}\")\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Frequency-Based Encoding\n",
    "\n",
    "The __frequency-based encoding__, instead of boolean values, represents the control flow in a case with the frequency of each event class in the case. This is achieved with the `Declare4Py.Encodings.Aggregate.Aggregate` class by setting the categorical attributes and the `boolean` parameter to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "encoder = Aggregate(case_id_col=case_id_key, cat_cols=['concept:name', 'org:group'], boolean=False)\n",
    "enc_df = encoder.fit_transform(df)\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Aggregated Encoding\n",
    "\n",
    "The __aggregated encoding__ considers all events since the beginning of the case, but ignore the order of the events. In this case, several aggregation functions can be applied to the values that an event attribute has taken throughout the case. This is achieved with the `Declare4Py.Encodings.Aggregate.Aggregate` class by setting the categorical attributes, the numeric attributes, the `boolean` parameter to `False` and a list of functions to aggregate the numeric attributes, e.g., 'mean', 'max', 'min', 'sum', 'std'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "encoder = Aggregate(case_id_col=case_id_key, cat_cols=['concept:name', 'org:group'], num_cols=['CRP'], boolean=False, aggregation_functions=['min', 'mean', 'max'])\n",
    "enc_df = encoder.fit_transform(df)\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexed Encodings\n",
    "\n",
    "#### The Simple-Index Encoding\n",
    "\n",
    "Another way of encoding a sequence is by taking into account also information about the order in which events occur in the sequence, as in the __simple-index encoding__. Here, each feature corresponds to a position in the sequence and the possible values for each feature are the presence of that event classes. This is achieved with the `Declare4Py.Encodings.IndexBased.IndexBased` class by setting the categorical attributes, the `create_dummies` parameter to `True` and the `max_events` to an integer value lower or equal than the maximum number of events in a trace in the log. If None, the parameter will set to the maximum number of events in a trace in the log. Such parameter sets the first events in the log to be use for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# with max_events the maximum number of events in a trace in the log.\n",
    "encoder = IndexBased(case_id_col=case_id_key, cat_cols=['concept:name'], create_dummies=True)\n",
    "enc_df = encoder.fit_transform(df)\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "# with max_events equal to 2.\n",
    "encoder = IndexBased(case_id_col=case_id_key, cat_cols=['concept:name'], max_events=2, create_dummies=True)\n",
    "enc_df = encoder.fit_transform(df)\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Complex-Index Encoding\n",
    "\n",
    "The __complex-based encoding__ takes into account also payload columns in the `cat_cols` or `num_cols`  parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "encoder = IndexBased(case_id_col=case_id_key, cat_cols = ['concept:name', 'org:group'], num_cols=['CRP'], create_dummies=True)\n",
    "enc_df = encoder.fit_transform(df)\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Encodings\n",
    "\n",
    "In a static encoding, only an available snapshot of the data is used. Therefore, the size of the feature vector is proportional to the number of event attributes and is fixed throughout the execution of a case.\n",
    "\n",
    "Using the last state abstraction, only one value (e.g., the last snapshot) of each data attribute is available. Here, the numeric attributes are added to the feature vector \"as is\" while one hot encoding is applied to each categorical attribute.\n",
    "\n",
    "#### The First-State Encoding\n",
    "In the __first-state encoding__ only the information (control flow and payload) of the first event is retained. This is achieved with the `Declare4Py.Encodings.Static.Static` class by setting the categorical and numeric attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "encoder = Static(case_id_col=case_id_key, cat_cols = ['concept:name', 'org:group'], num_cols=['CRP'])\n",
    "enc_df = encoder.fit_transform(df)\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Second-to-Last-State Encoding\n",
    "\n",
    "In the __second-to-last-state encoding__ only the information (control flow and payload) of the second-to-last event is retained. This is achieved with the `Declare4Py.Encodings.PreviousState.PreviousState` class by setting the categorical and numeric attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "encoder = PreviousState(case_id_col=case_id_key, cat_cols = ['concept:name', 'org:group'], num_cols=['CRP'])\n",
    "enc_df = encoder.fit_transform(df)\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Last-State Encoding\n",
    "\n",
    "In the __last-state encoding__ only the information (control flow and payload) of the last event is retained. This is achieved with the `Declare4Py.Encodings.LastState.LastState` class by setting the categorical and numeric attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "encoder = LastState(case_id_col=case_id_key, cat_cols = ['concept:name', 'org:group'], num_cols=['CRP'])\n",
    "enc_df = encoder.fit_transform(df)\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ngram encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "encoder = Ngram(case_id_col=case_id_key, n=2 , v=0.7, act_col='concept:name')\n",
    "enc_df = encoder.fit_transform(df)\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Declare encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "encoder = DeclareTransformer(case_id_col=case_id_key, n=3 , v= 0.7, act_col='concept:name')\n",
    "enc_df = encoder.fit_transform(df)\n",
    "enc_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding combinations\n",
    "\n",
    "### The Index-Latest-Payload Encoding\n",
    "\n",
    "The index latest-payload encoding adds the lat- est encoding to the simple-index encoding.\n",
    "\n",
    "combination of a index-based encoding with a static one (the last state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "last_state_encoder = LastStateTransformer(case_id_col=case_id_key, cat_cols=['org:group'], num_cols=[])\n",
    "latest_df = last_state_encoder.fit_transform(df)\n",
    "\n",
    "simple_index_encoder = IndexBasedTransformer(case_id_col=case_id_key, cat_cols=['concept:name'], num_cols=[], create_dummies=True)\n",
    "simple_df = simple_index_encoder.fit_transform(df)\n",
    "\n",
    "index_latest_payload_df = pd.concat([latest_df, simple_df], axis=1)\n",
    "index_latest_payload_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Machine Learning pipeline\n",
    "\n",
    "\n",
    "Esempio di pipeline per variant discovery basata su CF\n",
    "\n",
    "### TODO: mettere in un df trace id e label\n",
    "### TODO fare clustering su varianti\n",
    "### TODO mostra 2 tracce con stesse label hanno variante simile, e due classi con lbl diversa hanno diverse varianti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "variants_discovery = Pipeline([('vect', Aggregate(case_id_col=case_id_key, cat_cols=['concept:name'], num_cols=[], boolean=True)),\n",
    "                              ('kmeans', KMeans(n_clusters=3, random_state=0))])\n",
    "variants_discovery.fit_transform(df)\n",
    "\n",
    "for label in discover_variants['kmeans'].labels_:\n",
    "    print(label)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b13726099ff4a9270d97cd5a303046c40236cea9d4b3d3acf7f22861afad882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
